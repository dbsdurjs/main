{
  "cells": [
    {
      "metadata": {
        "id": "TMG3IgcdtzdI"
      },
      "cell_type": "markdown",
      "source": [
        "# LLM Comparator: Running Comparative Evaluations with Google Vertex AI"
      ]
    },
    {
      "metadata": {
        "id": "Lu90H8Os4UFZ"
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kunEkAgFB9Yt"
      },
      "outputs": [],
      "source": [
        "#@title Install the LLM Comparator package\n",
        "! pip install llm_comparator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZlVpN83nJBv"
      },
      "outputs": [],
      "source": [
        "#@title Import relevant packages\n",
        "import vertexai\n",
        "from google.colab import auth\n",
        "\n",
        "# The comparison library provides the primary API for running Comparative\n",
        "# Evaluations and generating the JSON files required by the LLM Comparator web\n",
        "# app.\n",
        "from llm_comparator import comparison\n",
        "\n",
        "# The model_helper library is used to initialize API wrapper to interface with\n",
        "# models. For this demo we focus on models served by Google Vertex AI, but you\n",
        "# can extend the llm_comparator.model_helper.GenerationModelHelper and\n",
        "# llm_comparator.model_helper.EmbeddingModelHelper classes to work with other\n",
        "# providers or models you host yourself.\n",
        "from llm_comparator import model_helper\n",
        "\n",
        "# The following libraries contain wrappers that implement the core functionality\n",
        "# of the Comparative Evaluation workflow. More on these below.\n",
        "from llm_comparator import llm_judge_runner\n",
        "from llm_comparator import rationale_bullet_generator\n",
        "from llm_comparator import rationale_cluster_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ygUJPVxDMB7"
      },
      "outputs": [],
      "source": [
        "#@title Setup and authenticate with Google Vertex AI.\n",
        "PROJECT_ID = 'your_project_id'  #@param {type: \"string\"}\n",
        "REGION = 'us-central1'  #@param {type: \"string\"}\n",
        "\n",
        "auth.authenticate_user()\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBGp4LiVCO00"
      },
      "outputs": [],
      "source": [
        "#@title Prepare Your Inputs\n",
        "\n",
        "# See llm_comparator.llm_judge_runner.LLMJudgeInput for the required input type.\n",
        "llm_judge_inputs = [\n",
        "    {'prompt': 'how are you?', 'response_a': 'good', 'response_b': 'bad'},\n",
        "    {'prompt': 'hello?', 'response_a': 'hello', 'response_b': 'hi'},\n",
        "    {'prompt': 'what is the capital of korea?', 'response_a': 'Seoul', 'response_b': 'Vancouver'}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy6GxkgJCD-M"
      },
      "outputs": [],
      "source": [
        "#@title Initialize models used in the LLM Comparator evaluation.\n",
        "\n",
        "# The generator model can be any Text-to-Text LLM provided by Vertex AI. This\n",
        "# model will be asked to do a series of tasks---judge, bulletize, and cluster---\n",
        "# and it is often beneficial to use a larger model for this reason.\n",
        "#\n",
        "# We default to 'gemini-pro' but you can change this with the `model_name=`\n",
        "# param. For a full list of models available via the Model Garden, check out\n",
        "# https://console.cloud.google.com/vertex-ai/model-garden?pageState=(%22galleryStateKey%22:(%22f%22:(%22g%22:%5B%22supportedTasks%22,%22inputTypes%22%5D,%22o%22:%5B%22GENERATION%22,%22LANGUAGE%22%5D),%22s%22:%22%22)).\n",
        "#\n",
        "# Since we're using Gemini Pro, a very competent and flexible foundation model,\n",
        "# we are sharing the same generator across all downstream tasks. However, you\n",
        "# could use different models for each task if desired.\n",
        "generator = model_helper.VertexGenerationModelHelper()\n",
        "\n",
        "# The embedding model can be any text embedder provided by Vertex AI. We default\n",
        "# to 'textembedding-gecko@003' but you can change this with the `model_name=`\n",
        "# param. For a full list of models available via the Model Garden, check out\n",
        "# https://console.cloud.google.com/vertex-ai/model-garden?pageState=(%22galleryStateKey%22:(%22f%22:(%22g%22:%5B%22supportedTasks%22,%22inputTypes%22%5D,%22o%22:%5B%22EMBEDDING%22,%22LANGUAGE%22%5D),%22s%22:%22%22))\n",
        "embedder = model_helper.VertexEmbeddingModelHelper()\n",
        "\n",
        "# The following models do the core work of a Comparative Evaluation: judge,\n",
        "# bulletize, and cluster. Each class provides a `.run()` function, and the\n",
        "# `llm_comparator.comparison.run()` API orchestrates configuring and calling\n",
        "# these APIs on the instances you pass in. More on how to configure these below.\n",
        "\n",
        "# The `judge` is the model responsible for actually doing the comparison between\n",
        "# the two models. The same judge is run multiple times to get a diversity of\n",
        "# perspectives, more on how to configure this below.\n",
        "#\n",
        "# A judge must phrase its responses in a simple XML format that includes the\n",
        "# verdict and an explanation of the results, to enable downstream processing by\n",
        "# the bulletizer and clusterer.\n",
        "#\n",
        "#     \u003cresult\u003e\n",
        "#       \u003cexplanation\u003eYOUR EXPLANATION GOES HERE.\u003c/explanation\u003e\n",
        "#       \u003cverdict\u003eA is slightly better\u003c/verdict\u003e\n",
        "#     \u003c/result\u003e\n",
        "#\n",
        "# We provide a default \"judge\" prompt in\n",
        "# llm_comparator.llm_judge_runner.DEFAULT_LLM_JUDGE_PROMPT_TEMPLATE, and you can\n",
        "# use the `llm_judge_prompt_template=` parameter to provide a custom prompt that\n",
        "# may better suit your needs.\n",
        "judge = llm_judge_runner.LLMJudgeRunner(generator)\n",
        "\n",
        "# The `bulletizer` condenses the results provided by the judge into a set of\n",
        "# bullets to make them easier to understand and consume in the UI.\n",
        "bulletizer = rationale_bullet_generator.RationaleBulletGenerator(generator)\n",
        "\n",
        "# The `clusterer` takes the bullets, embeds them, groups them into clusters\n",
        "# based on embedding similarity, and generates a label for those clusters.\n",
        "clusterer = rationale_cluster_generator.RationaleClusterGenerator(\n",
        "    generator, embedder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUU3V63vVbvS"
      },
      "outputs": [],
      "source": [
        "#@title Run the Comparative Evaluation.\n",
        "\n",
        "# The comparison.run() function is the primary interface for running a\n",
        "# Comparative Evaluation. It take your prepared inputs, a judge, a buletizer,\n",
        "# and a clusterer and returns a Python dictioary in the required format for use\n",
        "# in the LLM Comparator web app. You can inspect this dictionary in Python if\n",
        "# you like, but it's more useful once written to a file.\n",
        "#\n",
        "# The example below is basic, but you can use the judge_opts=, bulletizer_opts=,\n",
        "# and/or clusterer_opts= parameters (all of which are optional dictionaries that\n",
        "# are converted to keyword options) to further customize the behaviors. See the\n",
        "# Docsrtrings for more.\n",
        "comparison_result = comparison.run(\n",
        "    llm_judge_inputs,\n",
        "    judge,\n",
        "    bulletizer,\n",
        "    clusterer,\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "hViMkxUGhnTA"
      },
      "cell_type": "code",
      "source": [
        "#@title [Optional] Save the results to a file.\n",
        "file_path = 'json_for_llm_comparator.json' # @param {type: \"string\"}\n",
        "comparison.write(comparison_result, file_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7W4B1uzqvJZ-"
      },
      "cell_type": "code",
      "source": [
        "#@title [Optional] View the results in the app in Colab.\n",
        "comparison.show_in_colab(file_path)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/javascript/llm_comparator/python/notebooks/run_scripts_for_llm_comparator.ipynb?workspaceId=kahng:comparator12-2::citc",
          "timestamp": 1718335330333
        },
        {
          "file_id": "/piper/depot/google3/third_party/javascript/llm_comparator/python/notebooks/run_scripts_for_llm_comparator.ipynb?workspaceId=kahng:fig-export-comparator12-2-3818-change-2::citc",
          "timestamp": 1718334507750
        },
        {
          "file_id": "/piper/depot/google3/third_party/javascript/llm_comparator/python/notebooks/run_scripts_for_llm_comparator.ipynb?workspaceId=kahng:fig-export-comparator12-2-3818-change-2::citc",
          "timestamp": 1718215678229
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
